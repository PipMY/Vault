## 1. Elementary Functions

We should know and be able to prove all of the following equations. $$
\log_{b}(xy)=\log_{b}(x)+\log_{b}(y)
$$ $$
\log_{b}\left( \frac{x}{y} \right)=\log_{b}(x)-\log_{b}(y)
$$$$
\log_{b}(x^y)=y \cdot \log_{b}(x)
$$$$
\log_{b}(b^x)=x
$$ $$
b^{a\cdot \log_{b}x}=x^a
$$ $$
\log_{b}x=\frac{\log_{a}x}{\log_{a}b}
$$
---

## 2. Time complexity

The **time complexity** of an algorithm can be expressed in terms of the number of basic operations used by the algorithm when the input has a particular size. 

The **space complexity** of an algorithm is expressed in terms of the memory required by the algorithm for an input of a particular size. We will mainly be concerned with the time complexity. 

An example of when we should optimise the time complexity of an algorithm, given the polynomial $p(x)$: $$
p(x)=a_{n}x^n+a_{n-1}x^{n-1}+\dots+a_{2}x^2+a_{1}x+a_{0}
$$
The most straightforward way to compute this polynomial for $(x=x_{0})$, we would need $2n$ multiplications and $n$ additions as shown in the algorithm below:

```
power = 1 
y = a0
for i = 1 to n do
	power = power ∗ x0 
	y = y + a_ ∗ power
end for
```

The worst-case time complexity can be expressed in terms of the largest number of basic operations used by an algorithm when the input has a particular size. And usually the phrase time complexity means worst-case time complexity.

---

## 3. Big-O Notation

### Definition:

let $f$ and $g$ be functions from the set of integers or the set of real numbers to the set of real numbers. We say that $f$ is $O(g)$ if there are constants $C$ and $k$ such that: $$
|f(x)|\leq C \cdot|g(x)|
$$
whenever $x \geq k$.

What this means is that after a certain value of $x$ when $x\geq k$: 

- the absolute value of $f(x)$ is bounded from above by $C$ lots of the absolute value of $g(x)$. 
- In terms of time complexities $f(x)$ is no worse than $C \cdot g(x)$ for all relatively large input sizes $x$. 
- $C$ is a fixed constant usually depending on the choice of $k$. We are not allowed to increase $C$ as $x$ increases.

The constants $C$ and $k$ in the definition of big-O are called **witnesses** to the relationship $f(x)$ is $O(g(x))$. This gives rise to the idea that if there is a pair of witnesses to the relationship $f(x)$ is $O(g(x))$, then there are infinitely many pairs of witnesses to that relationship. So, if $C$ and $k$ are one pair of witnesses, then any pair $C'$ and $k'$, where $C \leq C'$ and $k \leq k'$, is also a pair of witnesses.

To show that $f(x)$ is $O(g(x))$ we need only one pair of witnesses, so we do not have to look for the "best" values of $C$ and $k$.

### Example 1. 

Let $f(x)=x^2+2x+1$. Prove that $f \in O(x^2)$

To prove any kind of question like this, we should first take the absolute value of $f(x)$ to make it simpler because then there are no negative signs (even though this question has no negative terms), we can do this because $|f(x)| \geq f(x)$ for all values of $x$. Next we can state that $x^2\geq x$ for all values of $x\geq1$ so we can upper bound all terms by $x^2$, then the equation would be as follows: $$
|f(x)|=x^2+2x+1\leq x^2+2x^2+1\cdot x^2
$$ $$
x^2+2x^2+x^2=4x^2
$$
And new if we apply our definition of big-O, we can see that $|f(x)|\leq C\cdot|g(x)|$, where $f(x)=x^2+2x+1$, $g(x)=x^2$, $C=4$ and true for all values of $x\geq k$, where $k=1$.

Therefore $f\in O(x^2)$.

### Example 2.

Let $f(x)=3x^3-7x^2-4x+2$. Prove that $f \in O(x^2)$. 

Following the general steps as last time:

1. $|f(x)|=3x^3+7x^2+4x+2 \leq 3x^3 +7x^3 +4x^3+2x^3 = 16x^3$
2. From this $C=16, k=1$
3. $|f(x)|\leq 16 \cdot |g(x)|$, for all $x\geq 1$

Therefore, $f \in O(x^3)$.

### Example 3. (Inverse of before)

Let $f(x)=3^x$. Prove that $f \not\in O(2^x)$

For this question we need to demonstrate that there is no such value of $C$ that satisfies our definition for big-O. To do this we can use the definition of big-O, where: $$
|f(x)|\leq C \cdot |g(x)| 
$$
In this case, $f(x)=3^x$ and $g(x)=2^x$ so it would follow that $\frac{3^x}{2^x}\leq C$
and since for all values of $x\geq 0$, $3^x \geq 2^x$ and for $x\geq 1$, $3^x>2^x$. Therefore, for relatively large values of $x$, $3^x\gg 2^x$. Therefore: $$
\lim_{ x \to \infty } \frac{3^x}{2^x} = \infty
$$ And it is obvious that $C$ cannot be equal to or greater than $\infty$. 

Therefore, $f\not\in O(2^x)$.

### Sum and Products Rules

##### Sum Rule:

If $f_{1}(x) \space is \space O(g_{1}(x))$ and $f_{2}(x) \space is \space O(g_{2}(x))$, then $f_{1}(x)+f_{2}(x) \space is \space O(max\{|g_{1}(x)|,|g_{2}(x)|\})$. 

All this means is that if you have a script and the first section is in $O(n^2)$ but the second section is in $O(n)$ the whole script will have a time complexity of $O(n^2)$.

##### Product Rule:

If $f_{1}(x) \space is \space O(g_{1}(x))$ and $f_{2}(x) \space is \space O(g_{2}(x)), \space then \space f_{1}(x)\cdot f_{2}(x) \space is \space O(g_{1}(x)\cdot g_{2}(x))$.

All this means is that if you have some code nested in another for example a for loop that runs $n$ times and another for loop in that, that runs $n$ times as well, the whole code is $O(n^2)$.

---

## 4. Big-Omega Notation

Whilst Big-O notation is very useful to find reasonable **upper bounds** for growth rates, it doesn't really help much if we want the best function that matches the growth rate. As a first step in this direction, we introduce a similar definition for lower bounds which is called Big-Omega notation.

### Definition:

Let $f(x)$ and $g(x)$ be functions from the set of real numbers to the set of real numbers. We say that $f(x)$ is $\Omega(g(x))$ if there are positive constants $C$ and $k$ such that: $$
|f(x)|\geq C\cdot|g(x)|
$$
Whenever $x>k$. Note that this implies that $f(x)$ is $\Omega(g(x))$ if and only if $g(x)$ is $O(f(x))$.

---

## 5. Theta Notation

### Definition:

Let $f(x)$ and $g(x)$ be functions from the set of real numbers to the set of real numbers. We say that $f(x)$ is $\Theta(g(x))$ if $f(x)$ is $O(g(x))$ and $f(x$) is $\Omega(g(x))$.

This is equivalent to saying that there are constants $C_{1},C_{2}$ and $k$ such that $|f(x)|\leq C_{1}\cdot|g(x)|$ and $|g(x)|\leq C_{2}\cdot|f(x)|$ whenever $x\geq k$. 

What this all means is that if you have a function $f(x)$ and it can be sandwiched between $C_{1}\cdot g(x)$ and $C_{2}\cdot g(x)$ then $f(x)$ is $\Theta(g(x))$.

---

## 6. Little-o Notation

### Definition:

Let $f(x)$ and $g(x)$ be functions from the set of real numbers to the set of real numbers. We say that $f(x)$ is $o(g(x))$ when: $$
\lim_{ x \to \infty } \frac{f(x)}{g(x)}=0 
$$
The reason why we use Little-o is because we want a tool for disregarding or neglecting smaller order terms for example when we have the function $f(x)=3x^2+7x+2$, we want to be able to disregard the $7x+2$ when $x$ gets arbitrarily large without feeling guilty. Little-o gives us the tool for this, by this definition requiring some basic calculus. A less fun more definition can be found below: $$
o(g)=\{f:\mathbb{N}\to\mathbb{N}
 \space | \space \forall C>\exists k>0: C\cdot f(n)<g(n)\space \forall n\geq k\}
$$
---

## 7. Sublinear Functions

A function is called **sublinear** if it grows slower than any linear function. With Little-o notation we can make this very precise. 

### Definition:

A function $f(x)$ is called sublinear if $f(x)$ is $o(x)$, so if $\lim_{ x \to \infty } \frac{f(x)}{x}=0$.

#### Example 1.

The function $f(x)=\frac{100}{\log(x)}$ is sublinear because: $$
\lim_{ x \to \infty } \frac{f(x)}{x}=\lim_{ x \to \infty } \frac{100}{x\cdot \log x} =0
$$
#### Example 2.

The function $f(x)=\frac{1}{2}x$ isn't sublinear because: $$
\lim_{ x \to \infty } \frac{f(x)}{x}=\lim_{ x \to \infty } \frac{\frac{1}{2}x}{x}=\frac{1}{2}  
$$
#### Example 3.

The function $f(x)=\sqrt[3]{ x^2 }$ is sublinear because: $$
\lim_{ x \to \infty } \frac{f(x)}{x}=\lim_{ x \to \infty } \frac{x^{\frac{2}{3}}}{x}=\lim_{ x \to \infty } x^{-\frac{1}{3}} =0  
$$
---

## 8. Little-omega

$f=\omega(g) \iff g=o(f)$

### Definition:

$\omega(g)=\{f:\mathbb{N}\to \mathbb{N} \space | \space \forall C > 0 \space \exists k > 0: f(n)>C\cdot g(n)\space\forall n\geq k\}$

What this means is that the function $f(x)$ will grow strictly faster than $g(x)$ if $f(x)$ is in $\omega(g(x))$. 

### Example.

$f(n)=n^2$ and $g(n)=n$.

We can compute the limit $\lim_{ n \to \infty } \frac{n^2}{n}=\infty$

So, $f(n)$ is in $\omega(n)$.

---

## 9. General Rules

### Theorem 1.

If $f_{1}(x)$ is $o(g(x))$ and $f_{2}(x)$ is $o(g(x))$, then $f_{1}(x)+f_{2}(x)$ is $o(g(x))$.

This theorem essentially means that if you have the two functions $f_{1}(x)$ and $f_{2}(x)$ that both grow strictly smaller than $g(x)$ then the addition of them will still grow smaller than $g(x)$.

### Theorem 2.

If $f_{1}(x)$ is $O(g(x))$ and $f_{2}(x)$ is $o(g(x))$, then $f_{1}(x)+f_{2}(x)$ is $O(g(x))$.

This theorem essentially means that if you have some function $f_{1}(x)$ that grows at most as fast as $g(x)$ and a function $f_{2}(x)$ that grows strictly slower than $g(x)$ then the addition of those two functions will grow at most as fast as $g(x)$.
### Theorem 3.

If $f_{1}(x)$ is $\Theta(g(x))$ and $f_{2}(x)$ is $o(g(x))$, then $f_{1}(x)+f_{2}(x)$ is $\Theta(g(x))$.

This theorem essentially means that if you have some function $f_{1}(x)$ that grows at the same rate as $g(x)$ and another function $f_{2}(x)$ that grows strictly slower than $g(x)$ then the addition of those two functions will grow at the same rate as $g(x)$.

---

## 10. Freedom

This is such a long section of ADS, please get yourself a treat for reading all this.
