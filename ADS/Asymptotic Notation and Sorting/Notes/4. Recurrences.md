
## 1. Intro

So far we've seen two types of algorithms: **iterative** and **recursive**. To analyse iterative algorithms:
- Look at loop structure
- Identify relevant operations
- Count those operations
You'll end up with some sort of sum (the more nested loops the more nested sums).

To analyse recursive algorithms, first of all note that most of them are actually hybrids between iterative and recursive. An example of this is the partition function in QuickSort is iterative, the rest of QuickSort is recursive). Let's take an example of MergeSort:
- Split input of size $n$ into two halves of equal size $\frac{n}{2}$ each
- Independently recurse into each half
- Merge the resulting sorted sequences

This will normally give you the recurrence, pretty much trivially: $$

T(n)\leq\left\{
  \begin{array}{ll}
    d & \text{if } n\leq c, for \text{ constants  c}, d>0 \\
     \underbrace{2\cdot T\left( \frac{n}{2} \right)}_{recursions}+    \underbrace{a\cdot n}_{merging}     & \text{otherwise}
  \end{array}
\right.

$$
Two time-tested methods for solving such recurrences:
- Induction (Guess, Substitute and Verify)
- Master Theorem

---
## 2. Solving Recurrences by Induction

Here there is only 2 steps
- "Guess" correct solution
- Verify base case and step

Here the former is art the latter is mathematics, we'll normally not spend too much time on the base case as when we're talking algorithms, it's quite clear that constant size input requires some constant number of steps. Interesting technical bit is step. 

Let's take for example MergeSort: $$
T(n)\leq\left\{
  \begin{array}{ll}
    d & \text{if } n\leq c, for \text{ constants  c}, d>0 \\
     2\cdot T\left( \frac{n}{2} \right)+    {a\cdot n}     & \text{otherwise}
  \end{array}
\right.
$$
So what we do is start with a relatively high order of n and slowly reduce it until it no longer works then we know the previous function is the best to bound this algorithm. Let's start with $O(n^2)$.

- So we're guessing that $T(n)\leq k\cdot n^2$ for some constant $k>0$.
- We can ignore the case of $n=1$ and only consider $n\geq 2$.
- Base case ~ recurrence said $T(n) \leq d$ if $n \leq c$, for constants $c$ and $d$. Make $k$ big enough: $$
d\leq kn^2 \space for \space 2\leq n\leq c \iff k\ge \frac{d}{2^2}=\frac{d}{4}
$$
- Then for all $2\le n \le c$, we have $T(n)\le d \le kn^2$.
- Next all we need to do is plug in our guess of $T(n)\le kn^2$: $$
T(n)\le 2T\left( \frac{n}{2} \right)+an
$$ $$
T(n)\le 2\left( k (\frac{n}{2})^2 \right)+an
$$
$$
T(n) \le \frac{kn^2}{2}+an
$$
$$
kn^2 \ge \frac{kn^2}{2}+an
$$
$$
\frac{kn}{2} \ge a \iff n \ge \frac{2a}{k}
$$
Therefore, choose any $k \ge \frac{2a}{n}$, then $T(n)\le kn^2$ holds. However here we can notice that as $n\to \infty$, $k$ can any non-negative value. This is not a tight bound at all so we should try another guess and if you recall the common Big-O functions, $O(n\log n)$ is lower in complexity than $O(n^2)$. So $O(n\log n)$ can be our next guess:

- So we're guessing that $T(n) \le kn\log_{2} n$ for some constant $k>0$.
- We can ignore the case $n=1$ and only consider $n \ge 2$.
- Base case ~ recurrence said $T(n)\le d$ if $n \le c$, for some constants $c$ and $d$. Make $k$ big enough: $$
d\leq kn\log n \space for \space 2\leq n\leq c \iff k\ge \frac{d}{2\log_{2}2}=\frac{d}{2}
$$
- Then for all $2\le n \le c$, we have $T(n)\le d \le kn\log_{n}n$.
- Next all we need to do is plug in our guess of $T(n)\le kn\log_{n} n$: $$
T(n)\le 2T\left( \frac{n}{2} \right)+an
$$ $$
T(n) \le 2k \frac{n}{2}\log_{2} \frac{n}{2}+an
$$ $$
T(n) \le kn(\log_{2}n-\log_{2}2)+an
$$ $$
T(n)\le kn \log_{2}n-kn+an
$$ $$
T(n) \le kn\log_{2}n,\ if \ kn \ge an \iff k\ge a
$$
Therefore, $T(n) \le kn\log_{2}n$ for some constant $k$. Now this has also worked but what if we don't intuitively know we should stop at $O(n\log n)$, we can test against $O(n)$ and we will find that it will boil down to: $$
T(n) \le kn+an \le kn \iff an \le 0
$$
And the if and only if section of this obviously cannot be true, as $a,n>0$. So it must be $T(n)\le kn\log n$.

---

## 3. Solving Recurrences Using the Master Theorem 

This can be used if the recurrence is of the form $T(n)=aT\left( \frac{n}{b} \right)+f(n)$, for constants $a \ge 1$ and $b >1$.

There are 3 cases:
1. If $f(n)=O(n^{\log_{b}(a)-\epsilon})$ for some constant $\epsilon > 0$ **then** $T(n)=\Theta(n^{\log_{b}(a)})$
2. If $f(n)=\Theta(n^{\log_{b}(a)}\cdot \log^kn)$ with $k \ge 0$ **then** $T(n)=\Theta(n^{\log_{b}(a)}\log^{k+1}n)$.
3. If $f(n)=\Omega(n^{\log_{b}(a)+\epsilon})$ for some constant $\epsilon >0$ and if $af\left( \frac{n}{b} \right) \le cf(n)$ for some constant $c < 1$ and all $n$ large enough then $T(n)=\Theta(f(n))$.

### Example

For MergeSort, $f(n)=an$ so $f(n)=\Theta(n)$ (not the same $a$ that's used in our cases for master theorem). $a=b=2,k=0$, and we can apply the second case, giving $n^{\log_{b}a}=n^{\log_{2}2}=n$. so $T(n)=\Theta(n^{\log_{2}2}\log^{k+1}n)=\Theta(n\log n)$. Which is so much easier than the shenanigans we were doing before using induction.

---

## 4. Solving Recurrences: QuickSort

The recurrence for QuickSort looks like: $$
T(n)\leq\left\{
  \begin{array}{ll}
    d & \text{if } n\leq c, for \text{ constants  c}, d>0 \\
     T(q)+T(n-q-1)+an     & \text{for some 0} \le q < n
  \end{array}
\right.
$$
Here we can't use expansion nor can we use Master Theorem, not easily, anyway.

In the worst-case scenario (e.g., choosing the smallest or largest element as pivot each time), QuickSort's recurrence is:

$$
T_w(n) = \max_{0 \le q < n} \left\{ T_w(q) + T_w(n - q - 1) \right\} + a n
$$

To upper bound this, we guess that $T_w(n) \leq \alpha n^2$, for some constant $\alpha > 0$. Plugging the guess into the recurrence:

$$T_w(n) \leq \max_{0 \le q < n} \left\{ \alpha q^2 + \alpha (n - q - 1)^2 \right\} + a n
$$
$$
= \alpha \cdot \max_{0 \le q < n} \left\{ q^2 + (n - q - 1)^2 \right\} + a n
$$

We now simplify the expression inside the max:
$$
f(q) = q^2 + (n - q - 1)^2 
$$
$$= q^2 + (n - q - 1)^2 $$
$$= q^2 + n^2 - 2nq - 2n + q^2 + 2q + 1 $$
$$= 2q^2 - 2nq + 2q + n^2 - 2n + 1 $$

This expression is maximized at $q = 0$ or $q = n - 1$, so:

$$
\max_{0 \le q < n} \left\{ q^2 + (n - q - 1)^2 \right\} = (n - 1)^2
$$
Therefore,
$$T_w(n) \leq \alpha(n - 1)^2 + a n$$

$$T_w(n) \leq \alpha(n^2 - 2n + 1) + a n = \alpha n^2 - 2 \alpha n + \alpha + a n$$
$$= \alpha n^2 - (2\alpha n - a n - \alpha)$$

To ensure this is still $\leq \alpha n^2$, we require:

$$2\alpha n - a n - \alpha \geq 0$$

$$\Rightarrow \alpha (2n - 1) \geq a n
\quad \Rightarrow \quad \alpha \geq \frac{a n}{2n - 1}$$

As $n$ grows, $\frac{a n}{2n - 1} = \Theta(1)$, so this is satisfied for some constant $\alpha$.

Thus, the worst-case time complexity of QuickSort is:

$$\boxed{T_w(n) = \Theta(n^2)}$$
